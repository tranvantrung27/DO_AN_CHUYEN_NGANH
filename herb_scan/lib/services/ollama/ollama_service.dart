import 'dart:convert';
import 'package:flutter/foundation.dart' show debugPrint;
import 'package:http/http.dart' as http;
import '../../config/ollama_config.dart';

/// K·∫øt qu·∫£ t·ª´ Ollama API
class OllamaResponse {
  final bool success;
  final String? response;
  final String? error;

  OllamaResponse({
    required this.success,
    this.response,
    this.error,
  });
}

/// Service ƒë·ªÉ k·∫øt n·ªëi v·ªõi Ollama server v√† chat v·ªõi AI model
class OllamaService {
  static final OllamaService _instance = OllamaService._internal();
  factory OllamaService() => _instance;
  OllamaService._internal();

  /// G·ª≠i message ƒë·∫øn Ollama v√† nh·∫≠n ph·∫£n h·ªìi
  /// 
  /// [prompt] - C√¢u h·ªèi/c√¢u l·ªánh t·ª´ ng∆∞·ªùi d√πng
  /// 
  /// Tr·∫£ v·ªÅ [OllamaResponse] ch·ª©a k·∫øt qu·∫£ ho·∫∑c l·ªói
  Future<OllamaResponse> sendMessage(String prompt) async {
    try {
      debugPrint('ü§ñ [Ollama] G·ª≠i message: $prompt');

      final response = await http
          .post(
            Uri.parse(OllamaConfig.generateEndpoint),
            headers: {
              'Content-Type': 'application/json',
            },
            body: jsonEncode({
              'model': OllamaConfig.modelName,
              'prompt': prompt,
              'stream': OllamaConfig.stream,
              'options': {
                'temperature': OllamaConfig.temperature,
              },
            }),
          )
          .timeout(
            Duration(seconds: OllamaConfig.requestTimeout),
          );

      if (response.statusCode == 200) {
        final data = jsonDecode(utf8.decode(response.bodyBytes));
        final botReply = data['response'] ?? 'L·ªói: Kh√¥ng c√≥ ph·∫£n h·ªìi t·ª´ AI';

        debugPrint('‚úÖ [Ollama] Nh·∫≠n ph·∫£n h·ªìi th√†nh c√¥ng');
        return OllamaResponse(
          success: true,
          response: botReply,
        );
      } else {
        // Parse error message t·ª´ Ollama
        String errorMsg = _parseOllamaError(response.statusCode, response.body);
        debugPrint('‚ùå [Ollama] L·ªói ${response.statusCode}: ${response.body}');
        return OllamaResponse(
          success: false,
          error: errorMsg,
        );
      }
    } catch (e) {
      debugPrint('‚ùå [Ollama] L·ªói k·∫øt n·ªëi: $e');
      
      String errorMessage;
      if (e.toString().contains('TimeoutException') ||
          e.toString().contains('timeout')) {
        errorMessage =
            'K·∫øt n·ªëi timeout. Vui l√≤ng ki·ªÉm tra:\n'
            '1. Ollama server ƒëang ch·∫°y (ollama serve)\n'
            '2. ƒê·ªãa ch·ªâ IP ƒë√∫ng trong config\n'
            '3. M√°y t√≠nh v√† ƒëi·ªán tho·∫°i c√πng m·∫°ng WiFi';
      } else if (e.toString().contains('Failed host lookup') ||
          e.toString().contains('Connection refused')) {
        errorMessage =
            'Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Ollama server.\n'
            'Vui l√≤ng ki·ªÉm tra:\n'
            '1. ƒê√£ ch·∫°y l·ªánh: set OLLAMA_HOST=0.0.0.0\n'
            '2. ƒê√£ ch·∫°y l·ªánh: ollama serve\n'
            '3. ƒê·ªãa ch·ªâ IP trong config ƒë√∫ng\n'
            '4. ƒêi·ªán tho·∫°i v√† m√°y t√≠nh c√πng m·∫°ng WiFi';
      } else {
        errorMessage = 'L·ªói: $e';
      }

      return OllamaResponse(
        success: false,
        error: errorMessage,
      );
    }
  }

  /// Ki·ªÉm tra k·∫øt n·ªëi v·ªõi Ollama server
  Future<bool> checkConnection() async {
    try {
      // Th·ª≠ ping m·ªôt request ƒë∆°n gi·∫£n
      final response = await http
          .get(Uri.parse('${OllamaConfig.baseUrl}/api/tags'))
          .timeout(const Duration(seconds: 5));

      return response.statusCode == 200;
    } catch (e) {
      debugPrint('‚ùå [Ollama] Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c: $e');
      return false;
    }
  }

  /// Parse error message t·ª´ Ollama ƒë·ªÉ hi·ªÉn th·ªã th√¢n thi·ªán h∆°n
  String _parseOllamaError(int statusCode, String body) {
    try {
      final errorJson = jsonDecode(body);
      final errorText = errorJson['error'] ?? errorJson['message'] ?? body;

      // Ki·ªÉm tra l·ªói thi·∫øu RAM
      if (errorText.toString().contains('memory') || 
          errorText.toString().contains('Memory') ||
          errorText.toString().contains('GiB')) {
        return _formatMemoryError(errorText.toString());
      }

      // Ki·ªÉm tra l·ªói model kh√¥ng t√¨m th·∫•y
      if (errorText.toString().contains('model') && 
          (errorText.toString().contains('not found') || 
           errorText.toString().contains('does not exist'))) {
        return '‚ö†Ô∏è Model "${OllamaConfig.modelName}" kh√¥ng t√¨m th·∫•y.\n\n'
            'Vui l√≤ng ki·ªÉm tra:\n'
            '1. Model ƒë√£ ƒë∆∞·ª£c load v√†o Ollama ch∆∞a\n'
            '2. T√™n model trong config ƒë√∫ng: ${OllamaConfig.modelName}\n'
            '3. Ch·∫°y: ollama list (ƒë·ªÉ xem danh s√°ch model)';
      }

      // Tr·∫£ v·ªÅ l·ªói g·ªëc n·∫øu kh√¥ng match
      return 'L·ªói Server (${statusCode}):\n$errorText';
    } catch (e) {
      // N·∫øu kh√¥ng parse ƒë∆∞·ª£c JSON, tr·∫£ v·ªÅ body g·ªëc
      return 'L·ªói Server (${statusCode}):\n$body';
    }
  }

  /// Format l·ªói memory ƒë·ªÉ d·ªÖ hi·ªÉu h∆°n
  String _formatMemoryError(String errorText) {
    // Extract s·ªë l∆∞·ª£ng RAM t·ª´ error message
    // V√≠ d·ª•: "model requires more system memory (4.1 GiB) than is available (3.7 GiB)"
    final requiredMatch = RegExp(r'requires.*?\((\d+\.?\d*)\s*GiB\)').firstMatch(errorText);
    final availableMatch = RegExp(r'available\s*\((\d+\.?\d*)\s*GiB\)').firstMatch(errorText);

    String requiredStr = requiredMatch?.group(1) ?? '?';
    String availableStr = availableMatch?.group(1) ?? '?';

    return '‚ö†Ô∏è Thi·∫øu RAM ƒë·ªÉ ch·∫°y model!\n\n'
        'Model c·∫ßn: ${requiredStr} GiB\n'
        'RAM kh·∫£ d·ª•ng: ${availableStr} GiB\n\n'
        'üí° Gi·∫£i ph√°p:\n'
        '1. ƒê√≥ng c√°c ·ª©ng d·ª•ng kh√°c ƒëang ch·∫°y\n'
        '2. D√πng model nh·ªè h∆°n (quantized)\n'
        '3. Gi·∫£m context length trong Ollama\n'
        '4. N√¢ng c·∫•p RAM m√°y t√≠nh\n\n'
        'Ho·∫∑c th·ª≠ model nh·ªè h∆°n nh∆∞: q4_0, q5_0';
  }
}

